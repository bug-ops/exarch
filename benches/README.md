# Exarch Benchmark Suite

Comprehensive performance benchmarks for the exarch archive extraction library.

## Performance Targets

| Operation | Target | Notes |
|-----------|--------|-------|
| TAR extraction | 500 MB/s | Throughput on uncompressed TAR |
| ZIP extraction | 300 MB/s | Throughput on stored ZIP |
| Path validation | < 1 us | Per entry validation |
| Symlink validation | < 5 us | Per symlink entry |
| Format detection | < 10 us | Per file |

## Quick Start

```bash
# 1. Generate benchmark fixtures (required first time)
./benches/fixtures/generate_fixtures.sh

# 2. Run all benchmarks
./benches/run_all.sh

# 3. View HTML report
open target/criterion/report/index.html
```

## Benchmark Types

### Rust Criterion Benchmarks

Located in `crates/exarch-core/benches/`:

- **extraction.rs** - Archive extraction throughput
  - TAR (uncompressed, gzip, bzip2, xz, zstd)
  - ZIP (stored, deflate)
  - 7z
  - Different sizes (1MB, 10MB, 100MB)
  - Different structures (many files, nested dirs)

- **creation.rs** - Archive creation throughput
  - TAR and ZIP creation
  - Compression level comparison
  - File count scaling

- **validation.rs** - Security validation performance
  - Path validation (< 1 us target)
  - Symlink validation
  - Hardlink validation
  - Compression ratio checks (zip bomb detection)
  - Entry validator orchestration

- **progress.rs** - Progress callback overhead measurement

### Comparison Benchmarks

- **compare_python.py** - exarch vs native Python tarfile/zipfile
- **compare_node.js** - exarch-rs vs tar-fs/adm-zip

## Running Benchmarks

### Full Benchmark Suite

```bash
./benches/run_all.sh
```

### Quick Benchmarks (fewer iterations)

```bash
./benches/run_all.sh --quick
```

### Rust Only

```bash
./benches/run_all.sh --rust-only
```

### Comparison Only

```bash
./benches/run_all.sh --compare
```

### Individual Benchmark

```bash
# Run from project root
cd crates/exarch-core

# Run specific benchmark file
cargo bench --bench extraction

# Run specific benchmark group
cargo bench --bench validation -- path_validation

# Run with baseline comparison
cargo bench -- --save-baseline main
cargo bench -- --baseline main
```

## Benchmark Fixtures

Generated by `./fixtures/generate_fixtures.sh`:

| Fixture | Description | Size |
|---------|-------------|------|
| small_files.* | 1000 files x 1KB | ~1 MB |
| medium_files.* | 100 files x 100KB | ~10 MB |
| large_file.* | 1 file x 100MB | 100 MB |
| compressible_large.* | Highly compressible 100MB | ~100 MB |
| nested_dirs.* | 20 levels deep, 3 files/level | ~60 KB |
| many_files.* | 10,000 tiny files | ~200 KB |
| mixed.* | Mixed file sizes | ~10.5 MB |

Formats generated: `.tar`, `.tar.gz`, `.tar.bz2`, `.tar.xz`, `.tar.zst`, `.zip`, `.7z`

### Regenerating Fixtures

```bash
rm -rf benches/fixtures/*.tar* benches/fixtures/*.zip benches/fixtures/*.7z
./benches/fixtures/generate_fixtures.sh
```

## Interpreting Results

### Criterion Reports

Criterion generates detailed HTML reports in `target/criterion/`:

- **Throughput** - MB/s or ops/s
- **Mean/Median** - Average execution time
- **Std Dev** - Variability
- **Outliers** - Unusual measurements
- **Change** - Regression/improvement vs baseline

### Performance Targets

Check the benchmark output against targets:

```
path_validation/simple:
  time: 800 ns (target: < 1000 ns) [PASS]

tar_extraction/large_100mb:
  throughput: 520 MB/s (target: 500 MB/s) [PASS]
```

### Regression Detection

```bash
# Save baseline
cargo bench -- --save-baseline before-change

# Make changes, then compare
cargo bench -- --baseline before-change

# Look for "Performance has regressed" warnings
```

## CI Integration

Add to CI workflow:

```yaml
- name: Run benchmarks
  run: |
    ./benches/fixtures/generate_fixtures.sh
    cargo bench -- --noplot

- name: Check for regressions
  run: |
    # Fail if any benchmark regressed by >10%
    cargo bench -- --baseline main | grep -q "regressed" && exit 1 || exit 0
```

## Adding New Benchmarks

1. Add benchmark function in appropriate file:

```rust
fn benchmark_new_feature(c: &mut Criterion) {
    let mut group = c.benchmark_group("new_feature");
    group.throughput(Throughput::Bytes(size));

    group.bench_function("variant_a", |b| {
        b.iter(|| {
            // Code to benchmark
        });
    });

    group.finish();
}
```

2. Add to criterion_group:

```rust
criterion_group!(
    benches,
    // ... existing benchmarks
    benchmark_new_feature,
);
```

3. If new fixture needed, update `fixtures/generate_fixtures.sh`

## Troubleshooting

### Fixtures not found

```bash
./benches/fixtures/generate_fixtures.sh
```

### Python benchmarks skipped

```bash
cd crates/exarch-python
maturin develop --release
```

### Node.js benchmarks skipped

```bash
cd crates/exarch-node
npm run build
npm install tar adm-zip  # For comparison
```

### High variance in results

- Close other applications
- Run multiple times
- Use `--warm-up-time 5` for longer warmup
- Consider `--measurement-time 10` for more samples

### Criterion version mismatch

Ensure workspace uses consistent criterion version:

```toml
# Cargo.toml
[workspace.dependencies]
criterion = "0.5"
```

## Files

```
benches/
├── README.md                  # This file
├── run_all.sh                # Main benchmark runner
├── compare_python.py         # Python comparison script
├── compare_node.js           # Node.js comparison script
├── fixtures/
│   ├── generate_fixtures.sh  # Fixture generator
│   └── *.tar, *.zip, *.7z    # Generated test archives
└── BENCHMARK_RESULTS.md      # Generated results (after run)

crates/exarch-core/benches/
├── extraction.rs             # Extraction benchmarks
├── creation.rs               # Creation benchmarks
├── validation.rs             # Validation benchmarks
└── progress.rs               # Progress callback benchmarks
```

## Related Documentation

- [Criterion User Guide](https://bheisler.github.io/criterion.rs/book/)
- [CLAUDE.md Performance Section](../CLAUDE.md#4-performance)
- [cargo-flamegraph](https://github.com/flamegraph-rs/flamegraph)
